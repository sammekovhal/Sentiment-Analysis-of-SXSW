{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Hackthon3-Model1.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"-S3JnV7vtBAu","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import re \n","from nltk.corpus import stopwords\n","from nltk.tokenize import TweetTokenizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from string import punctuation\n","from nltk.stem.snowball import SnowballStemmer\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import preprocessor as t\n","from sklearn.metrics import f1_score\n","import collections\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3wg5YcwbtBA1","colab_type":"code","colab":{}},"source":["def get_handler(x):\n","    return [w for wrd in re.findall(r'(#[\\w]+)|(@[\\w]+)|({[\\w]+})',x) for w in wrd if w != '']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ibevFnBmtBA5","colab_type":"code","colab":{}},"source":["def sentimnt_label(x):\n","    #   0: Negative, 1: Neutral, 2: Positive, 3: Can't Tell\n","    if x==0:\n","        x='Negative'\n","    elif x==1:\n","        x='Neutral'\n","    elif x==2:\n","        x='Positive'\n","    elif x==3:\n","        x=\"Can't Tell\"\n","    else:\n","        x=''\n","    return x\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oTJx_qqttBA-","colab_type":"code","colab":{}},"source":["def sentimnt_int(x):\n","    #   0: Negative, 1: Neutral, 2: Positive, 3: Can't Tell\n","    if x=='Negative':\n","        x=0\n","    elif x=='Neutral':\n","        x=1\n","    elif x=='Positive':\n","        x=2\n","    elif x==\"Can't Tell\":\n","        x=3\n","    else:\n","        x=''\n","    return x\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GlE6FQ9LtBBD","colab_type":"code","colab":{}},"source":["def twt_clean(x):\n","    # Manual Cleaning\n","    #x = re.sub(r'(#[\\w]+)|(@[\\w]+)|({[\\w]+})','',x)\n","\n","    #x = re.sub(r'#[a-zA-Z .,\\/]+ ','',x)\n","    #x = re.sub(r'(#[\\w]+)|(@[\\w]+)|({[\\w]+})','',x)\n","    x = x.replace(\"#\", \"\")\n","    x = x.replace(r\"&\\w+;\", \"\")\n","    x = re.sub(r'[^a-zA-Z .,]+','',x)\n","    \n","    \n","    HAPPY_EMO = r\" ([xX;:]-?[dD)]|:-?[\\)]|[;:][pP]) \"\n","    SAD_EMO = r\" (:'?[/|\\(]) \"\n","    \n","    x = x.replace(HAPPY_EMO, \" happyemoticons \")\n","    x = x.replace(SAD_EMO, \" sademoticons \")\n","\n","    # Library based Clean\n","    t.set_options(t.OPT.URL,t.OPT.RESERVED,t.OPT.MENTION)\n","    x=t.clean(x)\n","    x = x.lower()\n","    return x\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-nbLSpG2tBBH","colab_type":"code","colab":{}},"source":["def tweet_token(x):\n","    tweet_tokenizer = TweetTokenizer()\n","    return tweet_tokenizer.tokenize(x)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AQa34Nr_tBBK","colab_type":"code","colab":{}},"source":["def stop_clean(x):\n","    manual_stopword_list = ['``', \"'s\", \"...\", \"n't\",\"sxsw\",\"link\",\"rt\",\"quot\",\"vs\",\"ipad\",\"iphone\",\"google\",\"apple\",\"phone\",\"quot\",\"twitter\",\"app\",\"amp\"]\n","    stop_words = list(set(stopwords.words('english')))+list(punctuation)+manual_stopword_list \n","    res = []\n","    for i in x:\n","        if i not in stop_words and len(i)>2:\n","            res.append(i)\n","    return res            \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g1tTJbGKtBBO","colab_type":"code","colab":{}},"source":["def stem_fun(x):\n","    stemmer = SnowballStemmer(\"english\")\n","    res = []\n","    for i in x:\n","        res.append(stemmer.stem(i))\n","    return res"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rnTxpdc0tBBT","colab_type":"code","colab":{}},"source":["def gen_wordcloud(text):\n","    wordcloud = WordCloud(\n","        width = 300,\n","        height = 200,\n","        background_color = 'black').generate(str(text))\n","        \n","    plt.imshow(wordcloud, interpolation = 'bilinear')\n","    plt.axis('off')\n","    plt.tight_layout(pad=0)\n","    return plt\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"51MRRNoCtBBW","colab_type":"code","colab":{}},"source":["def model2(X,y,test):\n","    from sklearn.feature_extraction.text import CountVectorizer\n","    from sklearn.linear_model import LogisticRegression\n","\n","    count_vec = CountVectorizer(ngram_range=(1, 1),stop_words = 'english',max_features=7000)    \n","    X_c = count_vec.fit_transform(X)\n","    X_c = X_c.toarray()    \n","    test_t = count_vec.transform(test)\n","    test_t = test_t.toarray()\n","\n","    #X_train, X_test, y_train, y_test = train_test_split(X_c, y, test_size=0.10, random_state=0)\n","    \n","    clf_model = LogisticRegression(max_iter=300)\n","    clf = clf_model.fit(X_c, y)\n","    #acc = clf.score(X_test, y_test)\n","    #y_pred = clf.predict(X_test)    \n","    test_pred = clf.predict(test_t)    \n","    #f1 = f1_score(y_test, y_pred,average='micro')\n","    #print ('Model2 Accuracy: {}'.format(f1))\n","    return test_pred\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"POHonQUDtBBZ","colab_type":"code","colab":{}},"source":["def data_load(file_path,_type):\n","    \n","    '''\n","    # Data Loading\n","    '''\n","    data_frame = pd.read_csv(file_path)\n","\n","    data_frame.head()\n","    data_frame.columns  \n","    \n","    data_frame['tweet_len'] = data_frame['tweet'].str.len()\n","    #sns.countplot(x= 'tweet_len',data = train_tweet)\n","    \n","    \n","    '''\n","    # EDA Part\n","        1) Drop rows where tweet is NA\n","        2) Clean invalid char and convert string to lower \n","        3) Tokenize\n","        4) Stopword Removal\n","        5) Stemming Words\n","    \n","    '''\n","    \n","    if _type == 'train':\n","        data_frame.dropna(subset = [\"tweet\"], inplace=True)\n","        data_frame['sentiment_label'] = data_frame['sentiment'].apply(sentimnt_label)\n","\n","    data_frame['tweet_token'] = data_frame['tweet']\n","    \n","    data_frame['tweet_handler'] = data_frame['tweet_token'].apply(get_handler)\n","    data_frame['tweet_token'] = data_frame['tweet_token'].apply(twt_clean)\n","    \n","    data_frame['tweet_token'] = data_frame['tweet_token'].apply(tweet_token)\n","    \n","    data_frame['tweet_token'] = data_frame['tweet_token'].apply(stop_clean)\n","    \n","    #train_tweet['tweet_token_word'] = train_tweet['tweet_token'].apply(stem_fun)\n","    data_frame['tweet_token_word'] = data_frame['tweet_token'].apply(lemma_fun)\n","    data_frame['tweet_token_text'] = data_frame['tweet_token_word'].apply(lambda x: ' '.join(x))\n","    \n","    '''\n","    Word Cloud\n","    '''\n","    data_frame['type']=_type\n","    #gen_wordcloud(pos_words)\n","    #gen_wordcloud(neg_words)\n","    return data_frame\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EiBLjbUotBBc","colab_type":"text"},"source":["# Load Train Data"]},{"cell_type":"code","metadata":{"id":"Sl-GCUYXtBBd","colab_type":"code","colab":{}},"source":["file_path='file/data/train.csv'\n","train_tweet = data_load(file_path,'train')\n","X = train_tweet['tweet_token_text']\n","y = train_tweet['sentiment_label']\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b_uxZFG0tBBg","colab_type":"text"},"source":["# Load Test Data"]},{"cell_type":"code","metadata":{"id":"iJsED2Y4tBBh","colab_type":"code","colab":{}},"source":["file_path='file/data/test.csv'\n","train_tweet = data_load(file_path,'test')\n","test = train_tweet['tweet_token_text']\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"filUJj-ptBBl","colab_type":"text"},"source":["# Main Function"]},{"cell_type":"markdown","metadata":{"id":"XWylX4ZOtBBl","colab_type":"text"},"source":["'''For Calling different prediction model as gererating files'''"]},{"cell_type":"code","metadata":{"id":"gAVynmPStBBm","colab_type":"code","colab":{}},"source":["def main(i):\n","    file_path='file/data/train.csv'\n","    tweet = data_load(file_path,'train')\n","    X = tweet['tweet_token_text']\n","    y = tweet['sentiment_label']\n","    \n","    file_path='file/data/test.csv'\n","    tweet = data_load(file_path,'test')\n","    test = tweet['tweet_token_text']\n","    resp = model2(X,y,test)\n","    df = pd.DataFrame(data=resp)\n","    df = pd.concat([tweet['tweet_id'],df],axis=1)\n","\n","    df['sentiment'] = df[0].apply(sentimnt_int)\n","\n","    df.drop(0,inplace=True,axis=1)\n","\n","    df.to_csv('submission_'+str(i)+'.csv', index=False)\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GvaAtKl2tBBq","colab_type":"text"},"source":["# Call Main Function"]},{"cell_type":"code","metadata":{"id":"W51Md3hktBBq","colab_type":"code","colab":{}},"source":["main(i)"],"execution_count":null,"outputs":[]}]}